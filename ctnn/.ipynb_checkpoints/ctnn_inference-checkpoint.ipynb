{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaoran/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, callbacks, Model, layers, optimizers\n",
    "import numpy as np\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import cv2\n",
    "from math import ceil\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# from util_gray import DataGenerator\n",
    "print(tf.__version__)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "assert tf.config.experimental.get_memory_growth(physical_devices[0]) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6796\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1  # 2\n",
    "training = False\n",
    "dropout = 0.5\n",
    "char_txt_path = './char_std.txt'\n",
    "model_path = './ctnn_epoch.001-loss.0.78-val_loss.0.80.h5'\n",
    "\n",
    "\n",
    "with open(char_txt_path, mode='r', encoding='UTF-8') as wf:\n",
    "    ff = wf.readlines()\n",
    "    num_classes = len(ff)\n",
    "print(num_classes)\n",
    "\n",
    "char_list = []\n",
    "with open(char_txt_path, 'r', encoding='UTF-8') as f:\n",
    "    ff = f.readlines()\n",
    "    for i, char in enumerate(ff):\n",
    "        char = char.strip()\n",
    "        char_list.append(char)\n",
    "    \n",
    "char2id = {j:i for i, j in enumerate(char_list)}\n",
    "id2char = {i:j for i, j in enumerate(char_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method EncoderLayer.call of <__main__.EncoderLayer object at 0x7fc090221c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <__main__.EncoderLayer object at 0x7fc090221c88>>, which Python reported as:\n",
      "    def call(self, x, training):\n",
      "        # x.shape          : (batch_size, seq_len, dim=d_model)\n",
      "        # attn_output.shape: (batch_size, seq_len, d_model)\n",
      "        # out1.shape       : (batch_size, seq_len, d_model)\n",
      "#         attn_output, _ = self.mha(x, x, x, encoder_padding_mask)\n",
      "        attn_output, _ = self.mha(x, x, x)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layer_norm1(x + attn_output)\n",
      "        \n",
      "        # ffn_output.shape: (batch_size, seq_len, d_model)\n",
      "        # out2.shape      : (batch_size, seq_len, d_model)\n",
      "        ffn_output = self.ffn(out1)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layer_norm2(out1 + ffn_output)\n",
      "        \n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method EncoderLayer.call of <__main__.EncoderLayer object at 0x7fc090221c88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method EncoderLayer.call of <__main__.EncoderLayer object at 0x7fc090221c88>>, which Python reported as:\n",
      "    def call(self, x, training):\n",
      "        # x.shape          : (batch_size, seq_len, dim=d_model)\n",
      "        # attn_output.shape: (batch_size, seq_len, d_model)\n",
      "        # out1.shape       : (batch_size, seq_len, d_model)\n",
      "#         attn_output, _ = self.mha(x, x, x, encoder_padding_mask)\n",
      "        attn_output, _ = self.mha(x, x, x)\n",
      "        attn_output = self.dropout1(attn_output, training=training)\n",
      "        out1 = self.layer_norm1(x + attn_output)\n",
      "        \n",
      "        # ffn_output.shape: (batch_size, seq_len, d_model)\n",
      "        # out2.shape      : (batch_size, seq_len, d_model)\n",
      "        ffn_output = self.ffn(out1)\n",
      "        ffn_output = self.dropout2(ffn_output, training=training)\n",
      "        out2 = self.layer_norm2(out1 + ffn_output)\n",
      "        \n",
      "        return out2\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "(None, 35, 512)\n"
     ]
    }
   ],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000,\n",
    "                               (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def get_position_embedding(sentence_length, d_model):\n",
    "    angle_rads = get_angles(np.arange(sentence_length)[:, np.newaxis],\n",
    "                            np.arange(d_model)[np.newaxis, :],\n",
    "                            d_model)\n",
    "    # position_embedding.shape: [sentence_length, d_model]\n",
    "    position_embedding = np.zeros_like(angle_rads)\n",
    "    # sines.shape: [sentence_length, d_model / 2]\n",
    "    # cosines.shape: [sentence_length, d_model / 2]\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    position_embedding[:, 0::2] = sines\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    position_embedding[:, 1::2] = cosines\n",
    "    \n",
    "    # position_embedding.shape: [1, sentence_length, d_model]\n",
    "    position_embedding = position_embedding[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(position_embedding, dtype=tf.float32)\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    理论上:\n",
    "    x -> Wq0 -> q0\n",
    "    x -> Wk0 -> k0\n",
    "    x -> Wv0 -> v0\n",
    "    \n",
    "    实战中:\n",
    "    q -> Wq0 -> q0\n",
    "    k -> Wk0 -> k0\n",
    "    v -> Wv0 -> v0\n",
    "    \n",
    "    实战中技巧：\n",
    "    q -> Wq -> Q -> split -> q0, q1, q2...\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        \n",
    "        self.WQ = keras.layers.Dense(self.d_model)\n",
    "        self.WK = keras.layers.Dense(self.d_model)\n",
    "        self.WV = keras.layers.Dense(self.d_model)\n",
    "        \n",
    "        self.dense = keras.layers.Dense(self.d_model)\n",
    "        \n",
    "    # def scaled_dot_product_attention(q, k, v, mask):\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - q: shape == (..., seq_len_q, depth)\n",
    "        - k: shape == (..., seq_len_k, depth)\n",
    "        - v: shape == (..., seq_len_v, depth_v)\n",
    "        - seq_len_k == seq_len_v\n",
    "        - mask: shape == (..., seq_len_q, seq_len_k)\n",
    "        Returns:\n",
    "        - output: weighted sum\n",
    "        - attention_weights: weights of attention\n",
    "        \"\"\"\n",
    "\n",
    "        # matmul_qk.shape: (..., seq_len_q, seq_len_k)\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b = True)\n",
    "\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    #     if mask is not None:\n",
    "    #         # 使得在softmax后值趋近于0\n",
    "    #         scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        # attention_weights.shape: (..., seq_len_q, seq_len_k)\n",
    "        attention_weights = tf.nn.softmax(\n",
    "            scaled_attention_logits, axis = -1)\n",
    "\n",
    "        # output.shape: (..., seq_len_q, depth_v)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # x.shape: (batch_size, seq_len, d_model)\n",
    "        # d_model = num_heads * depth\n",
    "        # x -> (batch_size, num_heads, seq_len, depth)\n",
    "        \n",
    "        x = tf.reshape(x,\n",
    "                       (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "#     def call(self, q, k, v, mask):\n",
    "    def call(self, q, k, v):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        q = self.WQ(q) # q.shape: (batch_size, seq_len_q, d_model)\n",
    "        k = self.WK(k) # k.shape: (batch_size, seq_len_k, d_model)\n",
    "        v = self.WV(v) # v.shape: (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "        # q.shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        # k.shape: (batch_size, num_heads, seq_len_k, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        # v.shape: (batch_size, num_heads, seq_len_v, depth)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        \n",
    "        # scaled_attention_outputs.shape: (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        #                                                  scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention_outputs, attention_weights = self.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "        \n",
    "        # scaled_attention_outputs.shape: (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention_outputs = tf.transpose(\n",
    "            scaled_attention_outputs, perm = [0, 2, 1, 3])\n",
    "        # concat_attention.shape: (batch_size, seq_len_q, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention_outputs,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # output.shape : (batch_size, seq_len_q, d_model)\n",
    "        output = self.dense(concat_attention)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "def feed_forward_network(d_model, dff):\n",
    "    # dff: dim of feed forward network.\n",
    "    return keras.Sequential([\n",
    "        keras.layers.Dense(dff, activation='relu'),\n",
    "        keras.layers.Dense(d_model)\n",
    "    ])\n",
    "\n",
    "\n",
    "class EncoderLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    x -> self attention -> add & normalize & dropout\n",
    "      -> feed_forward -> add & normalize & dropout\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layer_norm1 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        self.layer_norm2 = keras.layers.LayerNormalization(\n",
    "            epsilon = 1e-6)\n",
    "        \n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "    \n",
    "#     def call(self, x, training, encoder_padding_mask):\n",
    "    def call(self, x, training):\n",
    "        # x.shape          : (batch_size, seq_len, dim=d_model)\n",
    "        # attn_output.shape: (batch_size, seq_len, d_model)\n",
    "        # out1.shape       : (batch_size, seq_len, d_model)\n",
    "#         attn_output, _ = self.mha(x, x, x, encoder_padding_mask)\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layer_norm1(x + attn_output)\n",
    "        \n",
    "        # ffn_output.shape: (batch_size, seq_len, d_model)\n",
    "        # out2.shape      : (batch_size, seq_len, d_model)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layer_norm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def conv(x, filters=64, kernel_size=(3,3), strides=1, padding='same', activation='relu'):\n",
    "    x = layers.Conv2D(filters, kernel_size, strides, padding)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def net(inputs):\n",
    "    x = conv(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)\n",
    "    x = conv(x, filters=128)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2), strides=2)(x)\n",
    "    x = conv(x, filters=256)\n",
    "    x = conv(x, filters=256)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,1), strides=(2,1))(x)\n",
    "    x = conv(x, filters=512)\n",
    "    x = conv(x, filters=512)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,1), strides=(2,1))(x)\n",
    "#     x = conv(x, filters=512, kernel_size=(3,3), strides=(2,2))\n",
    "    x = conv(x, filters=512, kernel_size=(3,3), strides=(2,2), activation='sigmoid')\n",
    "    return x\n",
    "\n",
    "def map_to_sequence(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    assert shape[1]==1\n",
    "    return keras.backend.squeeze(x, axis=1)\n",
    "\n",
    "class EncoderModel(keras.layers.Layer):\n",
    "    def __init__(self, num_layers=2, d_model=512, num_heads=8,\n",
    "                 dff=1024, time_step=35, rate=0.1):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff\n",
    "        self.time_step = time_step\n",
    "        self.rate = rate\n",
    "        \n",
    "        self.map_to_sequence = layers.Lambda(map_to_sequence)\n",
    "        # position_embedding.shape: (1, max_length, d_model)\n",
    "        self.position_embedding = get_position_embedding(self.time_step, self.d_model)\n",
    "        self.dropout = layers.Dropout(self.rate)\n",
    "\n",
    "        self.encoder_layers = [\n",
    "            EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate)\n",
    "            for _ in range(self.num_layers)]\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        # x.shape: (batch_size, 1, time_step, d_model)\n",
    "        x = self.map_to_sequence(x)\n",
    "        # x.shape: (batch_size, time_step, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.position_embedding\n",
    "        x = self.dropout(x, training = training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layers[i](x, training)\n",
    "        \n",
    "        # x.shape: (batch_size, time_step, d_model)\n",
    "        return x\n",
    "\n",
    "# inputs = Input(name='input_image', shape=(32,280,3))\n",
    "inputs = Input(name='input_image', shape=(32,None,1))\n",
    "\n",
    "y = net(inputs)\n",
    "\n",
    "y = EncoderModel(num_layers=num_layers)(y, training)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "y = layers.Dropout(dropout)(y, training=training)\n",
    "y = layers.Dense(num_classes, activation='softmax', name='FC_1')(y)\n",
    "\n",
    "predict_model = Model(inputs, y)\n",
    "\n",
    "predict_model.load_weights(filepath=model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/shaoran/work/jupyter_notebook/Crnn/ocr_test_rec/2390_3394_1186_2130_2043_3534_3557_553_.jpg', '/home/shaoran/work/jupyter_notebook/Crnn/ocr_test_rec/3495_828_1696_1210_2512_1913_1841_2866_.jpg', '/home/shaoran/work/jupyter_notebook/Crnn/ocr_test_rec/524_4650_2495_.jpg']\n"
     ]
    }
   ],
   "source": [
    "img_root = '/home/shaoran/work/jupyter_notebook/Crnn/ocr_test_rec'\n",
    "image_names = os.listdir(img_root)\n",
    "image_pathes = []\n",
    "for name in image_names:\n",
    "    name = os.path.join(img_root, name)\n",
    "    image_pathes.append(name)\n",
    "\n",
    "print(image_pathes[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_mask_32x70 = []\n",
    "# w0 = []\n",
    "# img_mask_32x280 = []\n",
    "# w1 = []\n",
    "# img_mask_32xlonger = []\n",
    "# w2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for path in image_pathes:\n",
    "#     img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "#     h,w = img.shape\n",
    "# #     print(h,w,c)\n",
    "#     scale = h / 32\n",
    "#     ww = int(w * scale)\n",
    "#     img = cv2.resize(img, (ww, 32))\n",
    "#     img = np.expand_dims(img, axis=-1)\n",
    "#     if ww > 280:\n",
    "#         img_mask_32xlonger.append(img)\n",
    "#         w2.append(ww)\n",
    "#     elif ww > 70:\n",
    "#         img_mask_32x280.append(img)\n",
    "#         w1.append(ww)\n",
    "#     else:\n",
    "#         img_mask_32x70.append(img)\n",
    "#         w0.append(ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(img_mask_32xlonger))\n",
    "# print(len(img_mask_32x280))\n",
    "# print(len(img_mask_32x70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# img_ = cv2.imread(image_pathes[0], cv2.IMREAD_GRAYSCALE)\n",
    "# img_ = cv2.resize(img_, (280,32))\n",
    "# i_ = np.expand_dims(img_, axis=0)\n",
    "# i_ = np.expand_dims(i_, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# out_ = predict_model.predict(i_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 35, 6796)\n",
      "WARNING:tensorflow:From /home/shaoran/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:5783: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "(1, 35, 6796)\n",
      "CPU times: user 2.79 s, sys: 577 ms, total: 3.37 s\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# inference time: 0.391s 0.4s\n",
    "\n",
    "# w_0 = max(max(w0), max(w1), max(w2))\n",
    "# img_mask = img_mask_32x70 + img_mask_32x280 + img_mask_32xlonger\n",
    "# print(len(img_mask))\n",
    "\n",
    "# mask_0 = np.full((len(img_mask), 32, w_0, 1), fill_value=255)\n",
    "# print('1')\n",
    "# for i, img in enumerate(img_mask):\n",
    "#     mask_0[i,:, :img.shape[1], :] = img\n",
    "# print('2')\n",
    "\n",
    "\n",
    "for path in image_pathes:\n",
    "    img = cv2.imread(image_pathes[0], cv2.IMREAD_GRAYSCALE)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    out = predict_model.predict(img)\n",
    "    out_0 = keras.backend.ctc_decode(out, input_length=[out.shape[1]]*out.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
